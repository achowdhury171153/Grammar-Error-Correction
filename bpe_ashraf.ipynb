{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpe@ashraf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ7xxR5513x_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "265124ba-739b-4acc-a31f-41d3f71e1d88"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/1c026f3aeafd26db30cb633d9915aae666a415179afa5943263e5dbd55a6/tokenizers-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAXy4OE0Zuzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIG_FILE_URL = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n",
        "\n",
        "# Let's download the file and save it somewhere\n",
        "from requests import get\n",
        "with open('big.txt', 'wb') as big_f:\n",
        "    response = get(BIG_FILE_URL, )\n",
        "    \n",
        "    if response.status_code == 200:  #It's a HTTP status code, it means \"OK\" \n",
        "        big_f.write(response.content)\n",
        "    else:\n",
        "        print(\"Unable to get the file: {}\".format(response.reason))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js1fqyjaZy4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
        "# the overall pipeline for various well-known tokenization algorithm. \n",
        "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Then we enable lower-casing and unicode-normalization\n",
        "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
        "# executed in order.\n",
        "tokenizer.normalizer = Sequence([\n",
        "    NFKC(),\n",
        "    Lowercase()\n",
        "])\n",
        "\n",
        "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNDFkrihZ1E-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c015e838-eedf-4c96-d13a-71d738379ca8"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
        "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
        "tokenizer.train(trainer, [\"big.txt\"])\n",
        "\n",
        "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trained vocab size: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25x5Ejd4Z3te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "baa83069-148f-4bd0-bb14-0f193d8e5c6f"
      },
      "source": [
        "# You will see the generated files in the output.\n",
        "tokenizer.model.save('.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCsQAUpDZ5qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d2b6a04b-9f61-4d4e-cc7c-b772122804a7"
      },
      "source": [
        "# Let's tokenizer a simple input\n",
        "tokenizer.model = BPE('vocab.json', 'merges.txt')\n",
        "encoding = tokenizer.encode(\"hello ashraf, alphabetically\")\n",
        "\n",
        "print(\"Encoded string: {}\".format(encoding.tokens))\n",
        "\n",
        "print(\"Encoded string: {}\".format(encoding.ids))\n",
        "\n",
        "decoded = tokenizer.decode(encoding.ids)\n",
        "print(\"Decoded string: {}\".format(decoded))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded string: ['Ġhell', 'o', 'Ġash', 'ra', 'f', ',', 'Ġalph', 'abet', 'ically']\n",
            "Encoded string: [16594, 78, 4275, 481, 69, 11, 21631, 24393, 2135]\n",
            "Decoded string:  hello ashraf, alphabetically\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}